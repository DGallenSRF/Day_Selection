---
title: "R Notebook"
output: html_notebook
---

Set working directory and load necessary packages.

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = "H:/Projects/11000/11155/TraffStudy/DataCollection/FreewayData/Detector Data/Volume Data")

##below are a list of packages required to run the markdown file
library(tidyverse)
library(lubridate)
library(gridExtra)
library(reshape2)
library(zoo)
library(imputeTS)
library(xts)
library(dygraphs)
library(d3heatmap)
library(ggthemes)
library(plotly)

```

Load the data for March. Load each file

```{r explore March}

dir('./March 18')

```

```{r load data for March,warning=FALSE}
path <- './March 18'

### list out all the csv files that start with TT_ and then have a number. This will avoid loading in the unwanted aggregated files or VMT files.
###'^_\\d.' returns all files that end with _some_digit.csv.
list_csv <- dir(path=path,pattern = '*_\\d.csv')

myfiles <- lapply(paste(path,'/',list_csv,sep=''),
                  function(x) read.csv(x,stringsAsFactors = FALSE))

files <- mapply(cbind,myfiles,'filename' = list_csv,SIMPLIFY = F)

dat <- bind_rows(files)

rm(myfiles,files)

#create a dataset that we will edit....
March_18_dat <- dat
```



```{r examine columns}

colnames(March_18_dat)
```

Lets fix the column headers first. There are no headers for the first 3 columns and also there appears to be a column at the end of the data frame "X.3"

```{r suymmary of dataset}
summary(March_18_dat$X.3)
```

All NAs.... remove.

```{r remove blank last column}

March_18_dat <- March_18_dat[,!names(March_18_dat) == "X.3"]

```

Focus on the first three columns

```{r examine first 3 rows}
str(March_18_dat[1:3])
```

Column 1 is the detector ID
Column 2 is the metric
Column 3 is the date.


```{r change first 3 column names}

colnames(March_18_dat)[1:3] <- c('Detector_ID','Metric','Date')

```

Now lets change the data types in the columns

```{r metric and Date_posixct}

March_18_dat$Metric <- as.factor(March_18_dat$Metric)
March_18_dat$Date_Posixct <- as.POSIXct(March_18_dat$Date,format="%Y/%m/%d")
March_18_dat$Detector_ID <- as.factor(March_18_dat$Detector_ID)
## Check that our dates worked out. We don't want any NAs.
table(March_18_dat$Date_Posixct)

```

There noon and midnight are labelled as strings. The rest of the columns are labelled as times formats (in strings). Lets change noon and midnight to the same format as the other times.

```{r change noon midnight names}

colnames(March_18_dat)[colnames(March_18_dat)%in%
                         c('X1.AM','X2.AM','X3.AM','X4.AM','X5.AM','X6.AM',
                           'X7.AM','X8.AM','X9.AM','X10.AM','X11.AM',
                           'X1.PM','X2.PM','X3.PM','X4.PM','X5.PM','X6.PM',
                           'X7.PM','X8.PM','X9.PM','X10.PM','X11.PM','Noon','Midnight')] <-
  c('X1.00.AM','X2.00.AM','X3.00.AM','X4.00.AM','X5.00.AM','X6.00.AM',
    'X7.00.AM','X8.00.AM','X9.00.AM','X10.00.AM','X11.00.AM',
    'X1.00.PM','X2.00.PM','X3.00.PM','X4.00.PM','X5.00.PM','X6.00.PM',
    'X7.00.PM','X8.00.PM','X9.00.PM','X10.00.PM','X11.00.PM','X00.00.AM','X00.00.PM')

March_18_dat <- March_18_dat %>% select(Detector_ID,Metric,Date,Date_Posixct,filename,everything())


```

Melt the data

```{r melt}

March_18_melt <- melt(March_18_dat,id.vars = c('Detector_ID','Metric','Date','Date_Posixct','filename'),variable.name = "Time",value.name = 'Metric_value')

```

Create Date_Time variable

```{r Date_Time}

#remove X
March_18_melt$Time <- gsub('X','',March_18_melt$Time)

March_18_melt$Date_Time <- as.POSIXct(paste(March_18_melt$Date,March_18_melt$Time),format="%Y/%m/%d %I.%M.%p")

March_18_melt$Hour <- hour(March_18_melt$Date_Time)

```

Filter to the PM Peak hours, 2pm to 7pm.

```{r Peak Hours}

PM_Peak <- c(14:19)

March_18_melt_PM_Peak <- March_18_melt %>%
  filter(Hour>=14)%>%
  filter(Hour<=19)

```

Lets find what detectors have bad values. 

```{r NA values}

var <- March_18_melt_PM_Peak %>% 
  group_by(Detector_ID)%>%
  summarise(NAs =sum(Metric_value<0))%>%
  arrange(NAs)%>%
  filter(NAs==0)%>%
  droplevels()
  
no_NAs <- as.vector(var$Detector_ID)

```

##Time Series

Lets pick Detector 70. We want to create a variable for each day and them average the volumes for each day.

We also only want Tuesday, Wednesday and Thursday. 

```{r det_70}

Det_70 <-  March_18_melt_PM_Peak %>%
  filter(Detector_ID==150)%>%
  filter(Metric=="Volume")%>%
  mutate(YearDay=yday(Date_Time),
         Weekday=weekdays(Date_Time,abbreviate=TRUE))%>%
  # filter(Weekday %in% c('Tue','Wed','Thu'))%>%
  group_by(Date_Posixct)%>%
  summarise(SumVol = sum(Metric_value))

head(Det_70)
```


Now we create a time series for the data

```{r Time Series,fig.width=9,fig.height=5}

Det_70_ts_xts <- xts(Det_70$SumVol,order.by = Det_70$Date_Posixct)

dygraph(xts(Det_70$SumVol,order.by = Det_70$Date_Posixct))%>%
  dySeries(label = 'Total Vol.')%>%
  dyRangeSelector()
```

```{r decompose data}
Det_70_ts <- ts(Det_70$SumVol,frequency=3)

Det_70_ts_deomp <- decompose(Det_70_ts)

Det_70_ts_stl <- stl(Det_70_ts,s.window = 'periodic')

midweek <- c('Tuesday','Wednesday','Thursday')

day_select <-  cbind(data.frame(Det_70_ts_stl$time.series[,1:3]),Date=Det_70$Date_Posixct,Weekday=weekdays(Det_70$Date_Posixct))%>%
  # mutate(Resid = abs(remainder))%>%
  filter(Weekday %in% midweek)%>%
  arrange(remainder)%>%
  print()

day_select$Date[day_select$remainder==median(day_select$remainder)]
```


```{r}
FM_TS_F$Date[FM_TS_F$remainder==median(FM_TS_F$remainder)]
```


Day selection is `r paste(day_select$Weekday[1],day_select$Date[1])`.

## Correlation

Show plots of volumes by day

```{r plot values}
y <- March_18_melt_PM_Peak %>%
  mutate(Weekday = weekdays(Date_Time,abbreviate=TRUE),
         YearDay=yday(Date_Time),
         MonthDay=day(Date_Time),
         HourMin = as.numeric(format(Date_Time,"%H.%M")))%>%
  filter(Metric=='Volume')%>%
  filter(Detector_ID%in%c(147:151))%>%
  filter(Weekday %in% c('Tue','Wed','Thu'))

  ggplot(y,aes(x=Metric_value,color=as.factor(Detector_ID)))+
  geom_density()+facet_wrap(~MonthDay,nrow=4,scales = 'free_x')
```


Create detector ~ day matrix

```{r heatmap for subset,fig.width=9}

levels <- last(levels(March_18_melt_PM_Peak$Detector_ID),100)

matrix <- March_18_melt_PM_Peak %>%
  mutate(Weekday = weekdays(Date_Time,abbreviate=TRUE),
         YearDay=yday(Date_Time),
         MonthDay=day(Date_Time),
         HourMin = as.numeric(format(Date_Time,"%H"))+
           as.numeric(format(Date_Time,"%M"))/60)%>%
  filter(Metric=='Volume')%>%
  filter(Detector_ID %in% levels)%>%
  filter(Detector_ID %in% no_NAs)%>%
  group_by(Date_Posixct,Detector_ID)%>%
  arrange(Date_Time)%>%
  mutate(cumSum = cumsum(Metric_value))%>%
  mutate(correlation = lm(cumSum~HourMin)$coefficients[2])%>%
  arrange(Date_Posixct)


matrix_spread <- matrix%>%
  group_by(Date_Posixct,Detector_ID)%>%
  summarise(correlation=first(correlation))%>%
  spread(Date_Posixct,correlation)


vector <- as.character(matrix_spread$Detector_ID)
matrix2 <- matrix_spread[,-1] %>% as.data.frame()
rownames(matrix2) <- vector
d3heatmap(matrix2,scale = 'column',Colv = 'as-is',colors = 'Blues')
```

Cluster Days

```{r}

day_SDev <- March_18_melt_PM_Peak %>%
  mutate(Weekday = weekdays(Date_Time,abbreviate=TRUE),
         YearDay=yday(Date_Time),
         MonthDay=day(Date_Time),
         HourMin = as.numeric(format(Date_Time,"%H.%M")))%>%
  filter(Metric=='Volume')%>%
  group_by(Detector_ID,Date_Posixct)%>%
  summarise(SumVol = sum(Metric_value))%>%
  group_by(Date_Posixct)%>%
  summarise(SDev = sd(SumVol))%>%
  mutate(weekDay=weekdays(Date_Posixct,abbreviate=TRUE))

sundays <-day_SDev$Date_Posixct[day_SDev$weekDay%in%c('Sun')]

ggplot(day_SDev) + 
  geom_point(aes(Date_Posixct,SDev)) + 
  geom_rug(aes(Date_Posixct,SDev)) + 
  geom_vline(xintercept = sundays,linetype='dotted') +
  theme_tufte(ticks = F) +
  xlab("STD of the total volume across all Detectors") + 
  ylab("Date") + 
  theme(axis.title.x = element_text(vjust=-0.5), axis.title.y = element_text(vjust=1))

```

Cumulative running total

```{r cumsum, fig.width=9}

g <- March_18_melt_PM_Peak %>%
  mutate(Weekday = weekdays(Date_Time,abbreviate=TRUE),
         YearDay=yday(Date_Time),
         MonthDay=day(Date_Time),
         HourMin = as.numeric(format(Date_Time,"%H"))+
           as.numeric(format(Date_Time,"%M"))/60)%>%
  filter(Detector_ID==4069 & Metric=='Volume')%>%
  # filter(YearDay==65 & Metric=='Volume')%>%
  # filter(Detector_ID %in% no_NAs)%>%
  # filter(Metric=='Volume')%>%
  droplevels()%>%
  group_by(YearDay)%>%
  arrange(YearDay,HourMin)%>%
  # group_by(Detector_ID,Date_Posixct)%>%
  mutate(cumSum = cumsum(Metric_value),
         correlation=lm(cumSum~HourMin)$coefficients[2])

h <- g%>%
  group_by(Date_Posixct)%>%
  summarise(Correlation=first(correlation))%>%
  mutate(wday = weekdays(Date_Posixct))%>%
  arrange(Date_Posixct)

plot <- ggplot(g) + 
  geom_line(aes(x=HourMin,y=cumSum,group=Date_Posixct,color=Weekday)) + 
  # geom_rug(aes(x=HourMin,y=cumSum)) + 
  # geom_smooth(aes(x=HourMin,y=diff))+
  theme_tufte(ticks = F) +
  xlab("Cumulative sum during peak hour") + 
  ylab("CumSum") + 
  theme(axis.title.x = element_text(vjust=-0.5), axis.title.y = element_text(vjust=1))  

ggplotly(plot)

plot2 <- ggplot(g) + 
  geom_line(aes(x=HourMin,y=Metric_value,group=Date_Posixct,color=Weekday)) + 
  # geom_rug(aes(x=HourMin,y=cumSum)) + 
  # geom_smooth(aes(x=HourMin,y=diff))+
  theme_tufte(ticks = F) +
  xlab("Cumulative sum during peak hour") + 
  ylab("CumSum") + 
  theme(axis.title.x = element_text(vjust=-0.5), axis.title.y = element_text(vjust=1))  

ggplotly(plot2)

```



